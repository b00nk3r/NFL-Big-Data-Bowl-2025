{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training for clustering pre-snap movements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load our preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_preprocessed = pd.read_csv('data/track_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying direction and orientation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two features that have values in degrees from 0 to 360, they are 'dir' and 'o'. The problem with this is that degrees are circular. For example, value 300 is closer to 0 than to 200. To take this into account let's use convert degrees to sin and cos pairs. This will ensure that values close to each other spatially are also close in the transformed feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert degrees to radians\n",
    "track_preprocessed_sin_cos = track_preprocessed\n",
    "track_preprocessed_sin_cos['dir_rad'] = np.deg2rad(track_preprocessed['dir'])\n",
    "track_preprocessed_sin_cos['o_rad'] = np.deg2rad(track_preprocessed['o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sin and cos\n",
    "track_preprocessed_sin_cos['sin_dir'] = np.sin(track_preprocessed_sin_cos['dir_rad'])\n",
    "track_preprocessed_sin_cos['cos_dir'] = np.cos(track_preprocessed_sin_cos['dir_rad'])\n",
    "track_preprocessed_sin_cos['sin_o'] = np.sin(track_preprocessed_sin_cos['o_rad'])\n",
    "track_preprocessed_sin_cos['cos_o'] = np.cos(track_preprocessed_sin_cos['o_rad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "track_preprocessed_sin_cos.drop(['dir', 'o', 'dir_rad', 'o_rad'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to prepare our data for model training. Let's first explore how the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gameId</th>\n",
       "      <th>playId</th>\n",
       "      <th>nflId</th>\n",
       "      <th>frameId</th>\n",
       "      <th>jerseyNumber</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>s</th>\n",
       "      <th>a</th>\n",
       "      <th>dis</th>\n",
       "      <th>sin_dir</th>\n",
       "      <th>cos_dir</th>\n",
       "      <th>sin_o</th>\n",
       "      <th>cos_o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.935150e+05</td>\n",
       "      <td>193515.000000</td>\n",
       "      <td>193515.000000</td>\n",
       "      <td>193515.000000</td>\n",
       "      <td>193515.000000</td>\n",
       "      <td>193515.000000</td>\n",
       "      <td>193515.000000</td>\n",
       "      <td>193515.000000</td>\n",
       "      <td>193515.000000</td>\n",
       "      <td>193515.000000</td>\n",
       "      <td>193515.000000</td>\n",
       "      <td>193515.000000</td>\n",
       "      <td>193515.000000</td>\n",
       "      <td>193515.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.022099e+09</td>\n",
       "      <td>1810.303827</td>\n",
       "      <td>48095.865861</td>\n",
       "      <td>87.798672</td>\n",
       "      <td>34.654817</td>\n",
       "      <td>-2.560307</td>\n",
       "      <td>-0.100292</td>\n",
       "      <td>1.840526</td>\n",
       "      <td>1.273277</td>\n",
       "      <td>0.183987</td>\n",
       "      <td>-0.058224</td>\n",
       "      <td>-0.003223</td>\n",
       "      <td>0.650092</td>\n",
       "      <td>0.004339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.109477e+03</td>\n",
       "      <td>1144.594837</td>\n",
       "      <td>4561.215261</td>\n",
       "      <td>32.568068</td>\n",
       "      <td>32.226856</td>\n",
       "      <td>1.155717</td>\n",
       "      <td>8.482461</td>\n",
       "      <td>2.210957</td>\n",
       "      <td>1.483284</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>0.504333</td>\n",
       "      <td>0.861541</td>\n",
       "      <td>0.441455</td>\n",
       "      <td>0.618452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.022091e+09</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>30842.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-10.029998</td>\n",
       "      <td>-23.959999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.022092e+09</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>44881.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>-2.780001</td>\n",
       "      <td>-7.230001</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>-0.330514</td>\n",
       "      <td>-0.965564</td>\n",
       "      <td>0.348081</td>\n",
       "      <td>-0.431299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.022101e+09</td>\n",
       "      <td>1683.000000</td>\n",
       "      <td>47836.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>-2.330001</td>\n",
       "      <td>-0.090000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.064184</td>\n",
       "      <td>-0.010995</td>\n",
       "      <td>0.896409</td>\n",
       "      <td>0.008028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.022102e+09</td>\n",
       "      <td>2757.000000</td>\n",
       "      <td>52608.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>-1.950000</td>\n",
       "      <td>7.130000</td>\n",
       "      <td>3.550000</td>\n",
       "      <td>2.140000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.166597</td>\n",
       "      <td>0.964695</td>\n",
       "      <td>0.986170</td>\n",
       "      <td>0.437508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.022111e+09</td>\n",
       "      <td>4686.000000</td>\n",
       "      <td>55157.000000</td>\n",
       "      <td>695.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>5.250002</td>\n",
       "      <td>23.370000</td>\n",
       "      <td>8.640000</td>\n",
       "      <td>12.040000</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             gameId         playId          nflId        frameId  \\\n",
       "count  1.935150e+05  193515.000000  193515.000000  193515.000000   \n",
       "mean   2.022099e+09    1810.303827   48095.865861      87.798672   \n",
       "std    6.109477e+03    1144.594837    4561.215261      32.568068   \n",
       "min    2.022091e+09      55.000000   30842.000000       1.000000   \n",
       "25%    2.022092e+09     800.000000   44881.000000      68.000000   \n",
       "50%    2.022101e+09    1683.000000   47836.000000      87.000000   \n",
       "75%    2.022102e+09    2757.000000   52608.000000     106.000000   \n",
       "max    2.022111e+09    4686.000000   55157.000000     695.000000   \n",
       "\n",
       "        jerseyNumber              x              y              s  \\\n",
       "count  193515.000000  193515.000000  193515.000000  193515.000000   \n",
       "mean       34.654817      -2.560307      -0.100292       1.840526   \n",
       "std        32.226856       1.155717       8.482461       2.210957   \n",
       "min         1.000000     -10.029998     -23.959999       0.000000   \n",
       "25%        11.000000      -2.780001      -7.230001       0.030000   \n",
       "50%        17.000000      -2.330001      -0.090000       0.490000   \n",
       "75%        81.000000      -1.950000       7.130000       3.550000   \n",
       "max        89.000000       5.250002      23.370000       8.640000   \n",
       "\n",
       "                   a            dis        sin_dir        cos_dir  \\\n",
       "count  193515.000000  193515.000000  193515.000000  193515.000000   \n",
       "mean        1.273277       0.183987      -0.058224      -0.003223   \n",
       "std         1.483284       0.218182       0.504333       0.861541   \n",
       "min         0.000000       0.000000      -1.000000      -1.000000   \n",
       "25%         0.030000       0.010000      -0.330514      -0.965564   \n",
       "50%         0.680000       0.050000      -0.064184      -0.010995   \n",
       "75%         2.140000       0.350000       0.166597       0.964695   \n",
       "max        12.040000       1.050000       1.000000       1.000000   \n",
       "\n",
       "               sin_o          cos_o  \n",
       "count  193515.000000  193515.000000  \n",
       "mean        0.650092       0.004339  \n",
       "std         0.441455       0.618452  \n",
       "min        -1.000000      -1.000000  \n",
       "25%         0.348081      -0.431299  \n",
       "50%         0.896409       0.008028  \n",
       "75%         0.986170       0.437508  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_preprocessed_sin_cos.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the data is not scaled yet. Let's scale it using Standard Scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_scale = ['x', 'y', 's', 'a', 'dis', 'sin_o', 'cos_o', 'sin_dir', 'cos_dir'] # Define columns to be sclaed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the feature data and transform it\n",
    "track_preprocessed_scaled = track_preprocessed_sin_cos\n",
    "track_preprocessed_scaled[columns_to_scale] = scaler.fit_transform(track_preprocessed_sin_cos[columns_to_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that the data is scaled now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gameId</th>\n",
       "      <th>playId</th>\n",
       "      <th>nflId</th>\n",
       "      <th>frameId</th>\n",
       "      <th>jerseyNumber</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>s</th>\n",
       "      <th>a</th>\n",
       "      <th>dis</th>\n",
       "      <th>sin_dir</th>\n",
       "      <th>cos_dir</th>\n",
       "      <th>sin_o</th>\n",
       "      <th>cos_o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.935150e+05</td>\n",
       "      <td>193515.000000</td>\n",
       "      <td>193515.000000</td>\n",
       "      <td>193515.000000</td>\n",
       "      <td>193515.000000</td>\n",
       "      <td>1.935150e+05</td>\n",
       "      <td>1.935150e+05</td>\n",
       "      <td>1.935150e+05</td>\n",
       "      <td>1.935150e+05</td>\n",
       "      <td>1.935150e+05</td>\n",
       "      <td>1.935150e+05</td>\n",
       "      <td>1.935150e+05</td>\n",
       "      <td>1.935150e+05</td>\n",
       "      <td>1.935150e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.022099e+09</td>\n",
       "      <td>1810.303827</td>\n",
       "      <td>48095.865861</td>\n",
       "      <td>87.798672</td>\n",
       "      <td>34.654817</td>\n",
       "      <td>-8.459760e-17</td>\n",
       "      <td>8.224767e-18</td>\n",
       "      <td>-5.022983e-17</td>\n",
       "      <td>-1.528558e-16</td>\n",
       "      <td>8.591944e-17</td>\n",
       "      <td>2.834607e-17</td>\n",
       "      <td>1.879947e-17</td>\n",
       "      <td>2.069410e-16</td>\n",
       "      <td>2.349933e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.109477e+03</td>\n",
       "      <td>1144.594837</td>\n",
       "      <td>4561.215261</td>\n",
       "      <td>32.568068</td>\n",
       "      <td>32.226856</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.022091e+09</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>30842.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-6.463272e+00</td>\n",
       "      <td>-2.812836e+00</td>\n",
       "      <td>-8.324589e-01</td>\n",
       "      <td>-8.584196e-01</td>\n",
       "      <td>-8.432727e-01</td>\n",
       "      <td>-1.867374e+00</td>\n",
       "      <td>-1.156973e+00</td>\n",
       "      <td>-3.737856e+00</td>\n",
       "      <td>-1.623961e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.022092e+09</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>44881.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>-1.900938e-01</td>\n",
       "      <td>-8.405258e-01</td>\n",
       "      <td>-8.188900e-01</td>\n",
       "      <td>-8.381942e-01</td>\n",
       "      <td>-7.974394e-01</td>\n",
       "      <td>-5.399035e-01</td>\n",
       "      <td>-1.117002e+00</td>\n",
       "      <td>-6.841262e-01</td>\n",
       "      <td>-7.044016e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.022101e+09</td>\n",
       "      <td>1683.000000</td>\n",
       "      <td>47836.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.992761e-01</td>\n",
       "      <td>1.213350e-03</td>\n",
       "      <td>-6.108348e-01</td>\n",
       "      <td>-3.999763e-01</td>\n",
       "      <td>-6.141059e-01</td>\n",
       "      <td>-1.181786e-02</td>\n",
       "      <td>-9.021155e-03</td>\n",
       "      <td>5.579688e-01</td>\n",
       "      <td>5.966192e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.022102e+09</td>\n",
       "      <td>2757.000000</td>\n",
       "      <td>52608.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>5.280784e-01</td>\n",
       "      <td>8.523836e-01</td>\n",
       "      <td>7.731848e-01</td>\n",
       "      <td>5.843285e-01</td>\n",
       "      <td>7.608949e-01</td>\n",
       "      <td>4.457788e-01</td>\n",
       "      <td>1.123477e+00</td>\n",
       "      <td>7.612993e-01</td>\n",
       "      <td>7.004114e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.022111e+09</td>\n",
       "      <td>4686.000000</td>\n",
       "      <td>55157.000000</td>\n",
       "      <td>695.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>6.757997e+00</td>\n",
       "      <td>2.766927e+00</td>\n",
       "      <td>3.075361e+00</td>\n",
       "      <td>7.258724e+00</td>\n",
       "      <td>3.969230e+00</td>\n",
       "      <td>2.098269e+00</td>\n",
       "      <td>1.164455e+00</td>\n",
       "      <td>7.926272e-01</td>\n",
       "      <td>1.609930e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             gameId         playId          nflId        frameId  \\\n",
       "count  1.935150e+05  193515.000000  193515.000000  193515.000000   \n",
       "mean   2.022099e+09    1810.303827   48095.865861      87.798672   \n",
       "std    6.109477e+03    1144.594837    4561.215261      32.568068   \n",
       "min    2.022091e+09      55.000000   30842.000000       1.000000   \n",
       "25%    2.022092e+09     800.000000   44881.000000      68.000000   \n",
       "50%    2.022101e+09    1683.000000   47836.000000      87.000000   \n",
       "75%    2.022102e+09    2757.000000   52608.000000     106.000000   \n",
       "max    2.022111e+09    4686.000000   55157.000000     695.000000   \n",
       "\n",
       "        jerseyNumber             x             y             s             a  \\\n",
       "count  193515.000000  1.935150e+05  1.935150e+05  1.935150e+05  1.935150e+05   \n",
       "mean       34.654817 -8.459760e-17  8.224767e-18 -5.022983e-17 -1.528558e-16   \n",
       "std        32.226856  1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00   \n",
       "min         1.000000 -6.463272e+00 -2.812836e+00 -8.324589e-01 -8.584196e-01   \n",
       "25%        11.000000 -1.900938e-01 -8.405258e-01 -8.188900e-01 -8.381942e-01   \n",
       "50%        17.000000  1.992761e-01  1.213350e-03 -6.108348e-01 -3.999763e-01   \n",
       "75%        81.000000  5.280784e-01  8.523836e-01  7.731848e-01  5.843285e-01   \n",
       "max        89.000000  6.757997e+00  2.766927e+00  3.075361e+00  7.258724e+00   \n",
       "\n",
       "                dis       sin_dir       cos_dir         sin_o         cos_o  \n",
       "count  1.935150e+05  1.935150e+05  1.935150e+05  1.935150e+05  1.935150e+05  \n",
       "mean   8.591944e-17  2.834607e-17  1.879947e-17  2.069410e-16  2.349933e-17  \n",
       "std    1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00  \n",
       "min   -8.432727e-01 -1.867374e+00 -1.156973e+00 -3.737856e+00 -1.623961e+00  \n",
       "25%   -7.974394e-01 -5.399035e-01 -1.117002e+00 -6.841262e-01 -7.044016e-01  \n",
       "50%   -6.141059e-01 -1.181786e-02 -9.021155e-03  5.579688e-01  5.966192e-03  \n",
       "75%    7.608949e-01  4.457788e-01  1.123477e+00  7.612993e-01  7.004114e-01  \n",
       "max    3.969230e+00  2.098269e+00  1.164455e+00  7.926272e-01  1.609930e+00  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_preprocessed_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go on now, but let's scale the data a bit more. Instead of giving the same weight to all of the features that we are planning to use, we will multiply some of them by scaling factor to give them weight. We will do it to speed and acceleration values, since we are mostly focused on the spatial trajectories, but removing speed and acceleartion completely also doesn't seem as a good idea. So we will still keep this feature, but will restrict its influence on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_preprocessed_scaled['s'] *= 0.5\n",
    "track_preprocessed_scaled['a'] *= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sequences of coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we will use is LSTM encoder-decoder, so we need our data in sequences format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by frameId and group\n",
    "track_preprocessed_sorted = track_preprocessed_scaled.sort_values(by=['gameId', 'playId', 'nflId', 'frameId'])\n",
    "track_preprocessed_grouped = track_preprocessed_sorted.groupby(['gameId', 'playId', 'nflId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [] # array of sequences\n",
    "labels = []  # array for labels\n",
    "\n",
    "for (groupId, playId, nflId), group in track_preprocessed_grouped:\n",
    "    # We will keep only following values\n",
    "    seq = group[['x', 'y', 's', 'a', 'sin_dir', 'cos_dir']].values  # shape (T, 6)\n",
    "    \n",
    "    sequences.append(seq)\n",
    "    labels.append((groupId, playId, nflId))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also created 'labels' array that stores ids for every sequence now. Later we will add there cluster labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have array of sequences. The problem is that we can have sequences of different length there. We know from preprocessing step that max length for sequence is 50 frames. So let's take sequences that are shorter than 50 frames and pad them with the first frame in the beginning. This will ensure that all the sequences have length 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences = []\n",
    "\n",
    "for seq in sequences:\n",
    "    T = len(seq)\n",
    "\n",
    "    padding_value = seq[0]\n",
    "    \n",
    "    padded = np.full((50, 6), padding_value)\n",
    "\n",
    "    padded[50 - T:] = seq  # place sequnce at the end\n",
    "    padded_sequences.append(padded)\n",
    "\n",
    "padded_sequences = np.array(padded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that every sequence has 50 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4372, 50, 6)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we've got 4372 sequences, each of length 50 and each frame has 6 features. We can move on to model training now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the model, we will need to split the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_sequences\n",
    "Y = padded_sequences # Targets are same as inputs in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=42) # Using 10% of data for test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Encoder-Decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fisrt need to train Encoder-Decoder model in order to be able to get a vector representing sequence, which we can then use for clustering. Let's create a function to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "\n",
    "    # Hyperparameters choice\n",
    "    units = hp.Choice('units', [16, 32])\n",
    "    lr = hp.Choice('lr', [1e-3, 1e-5])\n",
    "    dropout = hp.Float('dropout', 0.0, 0.5, step=0.1)\n",
    "\n",
    "    # Define the input layer with shape (50, 6)\n",
    "    encoder_inputs = keras.Input(shape=(50, 6), name=\"encoder_input\")\n",
    "\n",
    "    # Define the LSTM layer for the encoder\n",
    "    encoder_lstm = keras.layers.LSTM(\n",
    "        units,\n",
    "        return_sequences=False,\n",
    "        name=\"encoder_lstm\",\n",
    "        dropout=dropout,          \n",
    "        recurrent_dropout=dropout\n",
    "    )\n",
    "\n",
    "    # Pass the encoder inputs through the encoder LSTM\n",
    "    encoder_output = encoder_lstm(encoder_inputs)\n",
    "\n",
    "    # Repeat the encoder output to match the sequence length for the decoder\n",
    "    decoder_inputs = keras.layers.RepeatVector(50)(encoder_output)\n",
    "\n",
    "    # Define the LSTM layer for the decoder\n",
    "    decoder_lstm = keras.layers.LSTM(\n",
    "        units,                   \n",
    "        return_sequences=True,\n",
    "        name=\"decoder_lstm\",\n",
    "        dropout=dropout,          \n",
    "        recurrent_dropout=dropout \n",
    "    )\n",
    "\n",
    "    # Pass the repeated vector through the decoder LSTM to generate the output sequence\n",
    "    decoder_output_sequence = decoder_lstm(decoder_inputs)\n",
    "\n",
    "    # Apply a TimeDistributed Dense layer to produce the final output\n",
    "    decoder_dense = keras.layers.TimeDistributed(keras.layers.Dense(6), name=\"decoder_output\")\n",
    "    decoder_outputs = decoder_dense(decoder_output_sequence)\n",
    "\n",
    "    # Define the autoencoder model with encoder inputs and decoder outputs\n",
    "    autoencoder = keras.Model(encoder_inputs, decoder_outputs, name=\"seq2seq_autoencoder\")\n",
    "\n",
    "    # Initialize the Adam optimizer with the chosen learning rate and gradient clipping\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr, clipnorm=1.0)\n",
    "\n",
    "    # Compile the model with Mean Squared Error loss and the defined optimizer\n",
    "    autoencoder.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse'  \n",
    "    )\n",
    "\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define early stopping callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when the model is created and ready for training, we need to decide which hyperparamters to tune. We will stick to units, learning rate and dropout rate. The ranges for them are already defined in our function used for building. Let's perform Random Search to find the best set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 Complete [00h 04m 09s]\n",
      "val_loss: 0.621957004070282\n",
      "\n",
      "Best val_loss So Far: 0.06336793303489685\n",
      "Total elapsed time: 01h 10m 32s\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=15,\n",
    ")\n",
    "tuner.search(X_train, Y_train, epochs=100, validation_split=0.1, batch_size=32, shuffle=True, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the best model, so we don't have to train it again in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.save('models/autoencoder_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check which hyperparameters gave the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "units: 32\n",
      "lr: 0.001\n",
      "dropout: 0.0\n"
     ]
    }
   ],
   "source": [
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param, value in best_hp.values.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating at the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's evaluate the model on the test set. First, let's load and compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model\n",
    "autoencoder_model = keras.models.load_model('models/autoencoder_model.keras', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "autoencoder_model.compile(optimizer=optimizer, loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's look at the MSE on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0625  \n",
      "Test MSE Loss: 0.0636\n"
     ]
    }
   ],
   "source": [
    "test_loss = autoencoder_model.evaluate(X_test, Y_test, batch_size=32)\n",
    "print(f\"Test MSE Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test MSE is about the same as validation MSE, which indicates that the model is not overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nflevn311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
